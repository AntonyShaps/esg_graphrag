{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34761b7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde40388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu130\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText\n",
    "import fitz \n",
    "from tqdm import tqdm\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac69d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfa68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c81c977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bcb8d5",
   "metadata": {},
   "source": [
    "# Image extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc6c8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nvidia: 100%|██████████| 40/40 [00:03<00:00, 13.28page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished: 40 pages saved to ../data/imgs_nvidia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing meta: 100%|██████████| 74/74 [00:09<00:00,  7.88page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished: 74 pages saved to ../data/imgs_meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing google: 100%|██████████| 120/120 [00:09<00:00, 12.09page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished: 120 pages saved to ../data/imgs_google\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def transform_to_image(company_name):\n",
    "    \"\"\"\n",
    "    Convert PDF pages into PNG images with a progress bar.\n",
    "    \"\"\"\n",
    "    pdf_path = f\"../data/{company_name}.pdf\"\n",
    "    out_dir = f\"../data/imgs_{company_name}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    # tqdm over the pages\n",
    "    for i, page in enumerate(tqdm(doc, desc=f\"Processing {company_name}\", unit=\"page\")):\n",
    "        pix = page.get_pixmap(dpi=200)\n",
    "        out_path = os.path.join(out_dir, f\"page_{i+1}.png\")\n",
    "        pix.save(out_path)\n",
    "\n",
    "    print(f\"\\nFinished: {len(doc)} pages saved to {out_dir}\")\n",
    "\n",
    "transform_to_image(\"nvidia\")\n",
    "transform_to_image(\"meta\")\n",
    "transform_to_image(\"google\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb91b0",
   "metadata": {},
   "source": [
    "# OCR with nanonets-3B model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebb79b",
   "metadata": {},
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d43c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2c23ab9f384f9c9412d44a73f9d340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34242822cb534be38689b5f2ec892400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"nanonets/Nanonets-OCR2-3B\"\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_path, \n",
    "    dtype=\"auto\", \n",
    "    device_map=\"auto\", \n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb81802",
   "metadata": {},
   "source": [
    "## OCR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_page_with_nanonets_s(\n",
    "    image: Image.Image,\n",
    "    model,\n",
    "    processor,\n",
    "    max_new_tokens=4096,\n",
    "    user_prompt=None\n",
    "):\n",
    "    \"\"\"\n",
    "    OCR using Nanonets-OCR2-3B with correct generation slicing,\n",
    "    memory cleanup, and proper processor usage.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = (\n",
    "        \"\"\"Extract the text from the above document as if you were reading it naturally.\n",
    "        Return the tables as html. Return the equations in LaTeX representation.\n",
    "        If there is an image in the document and image caption is not present, add a small description inside <img></img>.\n",
    "        Page numbers as <page_number></page_number>.\n",
    "        For footers use <footer></footer>.\n",
    "        Use ☐ and ☑ for checkboxes.\"\"\"\n",
    "        if user_prompt is None else user_prompt\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": [ {\"type\": \"image\", \"image\": image},\n",
    "                                      {\"type\": \"text\", \"text\": prompt},],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Build chat prompt\n",
    "    chat = processor.apply_chat_template(messages,tokenize = False, add_generation_prompt=True)\n",
    "\n",
    "    # Prepare inputs\n",
    "    inputs = processor(text=chat, images=image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Run generation (no gradients)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    # Correct slicing: take tokens AFTER the prompt length\n",
    "    generated = output_ids[:, inputs.input_ids.shape[1]:]\n",
    "\n",
    "    # Decode\n",
    "    text_output = processor.batch_decode(\n",
    "        generated,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )[0]\n",
    "\n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return text_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0bad5f",
   "metadata": {},
   "source": [
    "## Text saving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_txt(company, text):\n",
    "    output_file = f\"../data/{company}_ocr.txt\"\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64bf1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_txt(\"nvidia\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c834173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<header>People, Diversity, and Inclusion</header>\n",
      "\n",
      "<img>A horizontal bar chart titled “FY25 Hiring Data*” showing the distribution of hires by age across global regions. The Americas segment (40.5 %) is the longest green bar, followed by EMEA (31.3 %), APAC (26.9 %), and India (18.7 %). A legend indicates that the first three bars represent the Americas, EMEA, and APAC regions respectively, while the fourth bar represents India.</img>\n",
      "\n",
      "*Numbers do not equal 100% due to rounding. We have increased focus on diversity recruiting, resulting in an increase in global female hiring in each channel. For additional diversity metrics, please see our Sustainability Indicators.\n",
      "\n",
      "<footer>NVIDIA Sustainability | People, Diversity, and Inclusion</footer>\n",
      "\n",
      "<page_number>15</page_number>\n",
      "\n",
      "\n",
      "# FY25 Hiring Data*\n",
      "\n",
      "## By Age (Global)\n",
      "\n",
      "- **20-30 Years:** 40.5%\n",
      "- **31-50 Years:** 52.9%\n",
      "- **51+ Years:** 6.5%\n",
      "- **No Data:** 0.1%\n",
      "\n",
      "## By Gender (Global)\n",
      "\n",
      "- **Men:** 70.2%\n",
      "- **Women:** 26.9%\n",
      "- **No Data:** 2.3%\n",
      "- **Not Disclosed:** 0.6%\n",
      "\n",
      "## By Region (Global)\n",
      "\n",
      "- **Americas:** 49.1%\n",
      "- **EMEA:** 22.0%\n",
      "- **APAC:** 16.5%\n",
      "- **India:** 12.4%\n",
      "\n",
      "## By Race/Ethnicity (United States of America)\n",
      "\n",
      "- **Asian:** 52.7%\n",
      "- **White:** 31.3%\n",
      "- **Decline to State:** 5.4%\n",
      "- **Black or African American:** 2.5%\n",
      "- **Native Hawaiian or Other Pacific Islander:** 0.1%\n",
      "- **American Indian or Alaska Native:** 0.1%\n",
      "- **Hispanic or Latino:** 4.8%\n",
      "- **Two or More Races:** 2.5%\n",
      "\n",
      "---\n",
      "\n",
      "### Message From Our CEO\n",
      "\n",
      "We have increased focus on diversity recruiting, resulting in an increase in global female hiring in each channel. For additional diversity metrics, please see our [Sustainability Indicators](Sustainability Indicators).\n",
      "\n",
      "### Energy, Efficiency, and Climate\n",
      "\n",
      "ProWoman (Israel), and Upreach (EMEA). We also attend conferences that serve diverse communities and host on-site events for historically underrepresented groups.\n",
      "\n",
      "### Product Value Chain\n",
      "\n",
      "We partner with NVIDIA’s 10 different community resource groups to enhance support programs based on targeted needs, including military leave, student loan repayment, gender affirmation support, enhanced health insurance coverage for members with developmental delays, and mental health counselor search tools.\n",
      "\n",
      "### Responsible Business\n",
      "\n",
      "We evaluate our benefit offerings annually to ensure employee needs are met and continuously seek feedback from employees to advance our support.\n",
      "\n",
      "### Sustainability Indicators\n",
      "\n",
      "Our employees’ well-being, physical, emotional, and financial health is a top priority, and we aim to support them by offering a suite of services where people can choose what works best for them.\n",
      "\n",
      "### Benefits and Compensation\n",
      "\n",
      "We offer comprehensive benefits to support our employees’ and their families’ physical health, well-being, and financial health. Programs include 401(k) programs in the U.S., statutory and supplemental pension programs outside the U.S., our employee stock purchase program, flexible work hours, and time off policies. We evaluate our benefit offerings globally and aim to provide comparable support across the regions where we operate.\n",
      "\n",
      "We offer tailored benefits based on the needs of our employees including continuing support for parents; both new birth parents and those who wish to become parents. We provide employees with benefits such as reimbursement for eligible adoption, surrogacy, and fertility treatment expenses. Our parental leave program, available to all employees, enables birth parents to take up to 22 weeks of paid leave. Non-birth parents can take up to 12 weeks of paid leave. To ease the transition back to work after their leave, all new parents also receive up to eight weeks of flex time and access to parenting support modules that provide tips and guidance.\n",
      "\n",
      "### Pay and Promotion\n",
      "\n",
      "We strive to provide equitable compensation and opportunities for advancement to all employees and to achieve promotion parity based on a variety of considerations. We perform an annual review of peer compensation in the markets we operate in, and track equity and parity in retention, promotions, and pay.\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"../data/imgs_nvidia/page_15.png\")\n",
    "result = ocr_page_with_nanonets_s(image, model, processor, max_new_tokens=15000)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esg-graphrag (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
